{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "import preprocessor as p\n",
    "import joblib\n",
    "import tweepy\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter user name: @trtworld\n"
     ]
    }
   ],
   "source": [
    "user_name = input(\"Please enter user name:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"qzWcxVXwd10Wqg9BfpAjAXEom\"\n",
    "consumer_secret = \"kc4INOKZgZ6E7ojWWnbtqFFVx6V7InWqX1Lxgb6KY4zTVdciCw\"\n",
    "access_token = \"323830961-Ejorvc54i7bx0nXtFsqrcvJ5oa4Nou3mDaE3TwI4\"\n",
    "access_token_secret = \"SmvbwW6r3uxDDZ5nvNe8K63bXBfpIbRVDcbngXeCwi8zc\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID = user_name\n",
    "user = api.get_user(userID)\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name = userID,count=200, tweet_mode=\"extended\")\n",
    "tweets = [[tweet.full_text] for tweet in new_tweets]\n",
    "df = pd.DataFrame(tweets, columns=[\"tweet_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    for i in range(len(df[\"tweet_text\"])):\n",
    "        df[\"tweet_text\"][i] = re.sub(r'(?is)#', '', (df[\"tweet_text\"][i]))\n",
    "        df[\"tweet_text\"][i] = p.clean(df[\"tweet_text\"][i])\n",
    "        df[\"tweet_text\"][i] = re.sub(r'[^\\w\\s]','',df[\"tweet_text\"][i])\n",
    "        df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(('new','year', 'happy', 'first', 'one', 'lets', 'best', 'via', 'two', 'three', 'amp'))\n",
    "    for i in range(len(df[\"tweet_text\"])):\n",
    "        word_tokens = word_tokenize(df[\"tweet_text\"][i].lower())\n",
    "        filtered_sentence = \"\"\n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                filtered_sentence += w\n",
    "                filtered_sentence += \" \"\n",
    "    filtered_sentence = filtered_sentence.strip()\n",
    "    df[\"tweet_text\"][i] = filtered_sentence\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    for i in range(len(df[\"tweet_text\"])):\n",
    "        sentence = \"\"\n",
    "        tweet = df[\"tweet_text\"][i].lower().split()\n",
    "        porter = nltk.SnowballStemmer('english')\n",
    "        WNlemma = WordNetLemmatizer()\n",
    "        for k in tweet:\n",
    "            stemming = porter.stem(k)\n",
    "            lem = WNlemma.lemmatize(stemming)\n",
    "            sentence += lem\n",
    "            sentence += \" \"\n",
    "        sentence = sentence.strip()\n",
    "        df[\"tweet_text\"][i] = sentence\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(df):\n",
    "    #vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "    #tfidf = pd.read_csv(\"preprocessed_data.csv\")\n",
    "    vectorizer = pickle.load(open(\"vector.pickel\", \"rb\"))\n",
    "    #loaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=joblib.load(\"tf_idf.sav\"))\n",
    "    #tfidf = transformer.fit_transform(loaded_vec.fit_transform(df[\"tweet_text\"]))\n",
    "    #vectorizer.fit(tfidf[\"tweet_text\"].values.astype('U'))\n",
    "    x_test = vectorizer.transform(df)\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020 put Turkish drones in the spotlight as th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>British artist Damien Hirst to display more th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A group of friends gifted their friend colourb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indonesian Sriwijaya Air plane with 62 people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tehran bans foreign companies from testing Cov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text\n",
       "0  2020 put Turkish drones in the spotlight as th...\n",
       "1  British artist Damien Hirst to display more th...\n",
       "2  A group of friends gifted their friend colourb...\n",
       "3  Indonesian Sriwijaya Air plane with 62 people ...\n",
       "4  Tehran bans foreign companies from testing Cov..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df[\"tweet_text\"])):\n",
    "    if df[\"tweet_text\"][i] == '':\n",
    "        df.drop([i], inplace=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.applymap(str.lower)\n",
    "pre_df = preprocessing(df)\n",
    "final_df = stop_words(pre_df)\n",
    "df = stemming(final_df)\n",
    "#df = vectorization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "with open(\"models.pckl\", \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            models.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['art', 'game', 'movie', 'music', 'politic', 'science', 'sport', 'technology', 'travel', 'TV']\n",
    "total = []\n",
    "for i in range(len(df[\"tweet_text\"])):\n",
    "    test = [df[\"tweet_text\"][i]]\n",
    "    test = vectorization(test)\n",
    "    result = []\n",
    "    final = []\n",
    "    for model in models:\n",
    "        result.append(model.predict_proba(test))\n",
    "    for k in range(len(result)):\n",
    "        final.append(result[k][0][1])\n",
    "    index = final.index(max(final))\n",
    "    total.append(categories[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data.setdefault(\"result_name\", [])\n",
    "data.setdefault(\"result_percentage\", [])\n",
    "\n",
    "for category in categories:\n",
    "    percentage = int(total.count(category) * 100 / len(total))\n",
    "    data[\"result_name\"].append(category)\n",
    "    data[\"result_percentage\"].append(percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
